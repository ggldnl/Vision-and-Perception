{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":863,"status":"ok","timestamp":1678702336602,"user":{"displayName":"Luca Maiano","userId":"08981433972456241292"},"user_tz":-60},"id":"A4oDHmPrbTLy"},"outputs":[],"source":["import numpy as npi\n","import cv2 as cv2\n","\n","from PIL import Image\n","from utils import plot_cv2_images\n","\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"7ER9dng2bTL0"},"source":["# Image stitching\n","\n","Image stitching or photo stitching is the process of combining multiple photographic images with overlapping fields of view to produce a segmented panorama or high-resolution image. Nowadays it is one of the most successful applications in computer vision, and in fact, you find this functionality in basically every modern smartphone. In this notebook, we will implement a solution to combine multiple images into a panorama image. Let's begin by importing two images. We call them *left* and *right*."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":446,"output_embedded_package_id":"1Ulj0wzcQZ9bVR_mrlFZDbq9728qXGIJR"},"executionInfo":{"elapsed":7282,"status":"ok","timestamp":1678702355253,"user":{"displayName":"Luca Maiano","userId":"08981433972456241292"},"user_tz":-60},"id":"Q04SVdhnbTL2","outputId":"9af354c7-24df-478c-dc0c-7fc9b5381b48"},"outputs":[],"source":["left = cv2.imread('../images/DSC02931.JPG')\n","right = cv2.imread('../images/DSC02932.JPG')\n","\n","left = cv2.cvtColor(left, cv2.COLOR_BGR2RGB)\n","right = cv2.cvtColor(right, cv2.COLOR_BGR2RGB)\n","\n","plot_cv2_images(left, right, hide_ticks=True)"]},{"cell_type":"markdown","metadata":{"id":"6dNXH1jYbTL2"},"source":["Given a pair of images like the ones above, we want to stitch them to create a panoramic scene. It is important to note that both images need to share some common region. Moreover, our solution has to be robust even if the pictures have differences in one or more of the following aspects:\n","- Scaling\n","- Angle\n","- Spacial position\n","- Capturing devices\n","\n","An initial approach could be to extract key points using an algorithm such as *Harris Corners*. Then, we could try to match the corresponding key points based on some measure of similarity like *Euclidean distance*. As we know, corners have one nice property: they are *invariant to rotation*. It means that, once we detect a corner, if we rotate an image, that corner will still be there.\n","However, what if we rotate then scale an image? In this situation, we would have a hard time because corners are not invariant to scale! \n","\n","To address this limitation, methods like **SIFT** use Difference of Gaussians (DoD). The idea is to apply DoD on differently scaled versions of the same image. It also uses the neighboring pixel information to find and refine key points and corresponding descriptors.\n","\n","Now let's see how to implement SIFT with OpenCV. It is interesting to know that SIFT was originally patented, but the patent expired in 2020. Let's start by constructing a SIFT object. We can pass different parameters to it which are optional and they are well explained in the [documentation](https://docs.opencv.org/3.4/d7/d60/classcv_1_1SIFT.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1678702355604,"user":{"displayName":"Luca Maiano","userId":"08981433972456241292"},"user_tz":-60},"id":"rVKYfwOKbTL3"},"outputs":[],"source":["sift = cv2.SIFT_create()"]},{"cell_type":"markdown","metadata":{"id":"iP0teJkCbTL4"},"source":["## 1. Keypoint detection"]},{"cell_type":"markdown","metadata":{"id":"GyiIySNObTL4"},"source":["We can use two methods to calculate a SIFT descriptor with OpenCV.\n","\n","- The `sift.detect()` function finds the *keypoints* in the images. You can pass a mask if you want to search only a part of image. Each keypoint is a special structure which has many attributes like its $(x,y)$ coordinates, size of the meaningful neighbourhood, angle which specifies its orientation, response that specifies strength of keypoints etc. Next, you can call `sift.compute()` which computes the *descriptors* from the keypoints we have found. It returns a description containing the image changes like translation and rotation that allow us to match same or similar keypoints on different images.\n","- If you didn't find keypoints, you can directly find keypoints and descriptors in a single step with the function, `sift.detectAndCompute()`. It returns the *keypoints* and a *$128$-dimensional feature vector* for each key point."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1678702355606,"user":{"displayName":"Luca Maiano","userId":"08981433972456241292"},"user_tz":-60},"id":"msYlYaVpbTL7"},"outputs":[],"source":["kp_left, des_left = sift.detectAndCompute(left, None)\n","kp_right, des_right = sift.detectAndCompute(right, None)"]},{"cell_type":"markdown","metadata":{"id":"PJdsB2EtbTL7"},"source":["We can easily visualize found keypoints with OpenCV using the `drawKeypoints` function ([documentation](https://docs.opencv.org/3.4/d4/d5d/group__features2d__draw.html#gab958f8900dd10f14316521c149a60433))."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":945,"output_embedded_package_id":"10fVZ3XFnZkHC8PkAbGAkajlLGoq8JAxl"},"executionInfo":{"elapsed":12313,"status":"ok","timestamp":1678702367914,"user":{"displayName":"Luca Maiano","userId":"08981433972456241292"},"user_tz":-60},"id":"MtX0_uwobTL9","outputId":"40487384-efbd-4a5b-c4c9-133a537cc016"},"outputs":[],"source":["keypoints_drawn_left = cv2.drawKeypoints(left, kp_left, None, color=(0, 0, 255))\n","keypoints_drawn_right = cv2.drawKeypoints(right, kp_right, None, color=(0, 0, 255))\n","\n","plot_cv2_images(left, keypoints_drawn_left, right, keypoints_drawn_right, titles=['Left', 'Left keypoints', 'Right', 'Right keypoints'])"]},{"cell_type":"markdown","metadata":{"id":"n7UzbTLWbTL-"},"source":["## 2. Feature matching"]},{"cell_type":"markdown","metadata":{"id":"hnfQscJMbTL-"},"source":["As we can see, we have a large number of features from both images. Now, we would like to compare the 2 sets of features and stick with the pairs that show more similarity. We can match the feature descriptors using the **Brute-Force (BF) matcher**. It takes the descriptor of one feature in the first set and matches it with all other features in the second set according to some **distance function**. For the BF matcher, first we have to create the BF matcher object using `BFMatcher()`. It takes two optional params. First one is `normType`, that specifies the distance measurement to be used. By default, it is `NORM_L2` that is good for SIFT, SURF etc (`NORM_L1` is also there). If you are using binary string based descriptors like ORB, BRIEF, BRISK etc, `NORM_HAMMING` should be used, which used Hamming distance as measurement. \n","\n","The `crossCheck` bool parameter indicates whether the two features have to match each other to be considered valid. In other words, for a pair of features $(f1, f2)$ to considered valid, $f1$ needs to match $f2$ and $f2$ has to match $f1$ as the closest match as well. \n","\n","Once we have a matcher object, two important methods are `BFMatcher.match()` and `BFMatcher.knnMatch()`. First one returns the best match. Second method returns $k$ best matches where $k$ is specified by the user. It may be useful when we need to do additional work on that.\n","\n","The BF matcher can be very slow for large datasets, therefore it can be useful to apply faster matching algorithms like **FLANN**. FLANN stands for *Fast Library for Approximate Nearest Neighbors*. It contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. In this notebook we will use the BF matcher, but you can read [this page](https://docs.opencv.org/3.4/dc/dc3/tutorial_py_matcher.html) to learn more about FLANN."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18723,"status":"ok","timestamp":1678702386633,"user":{"displayName":"Luca Maiano","userId":"08981433972456241292"},"user_tz":-60},"id":"YlFP9uaGbTL_"},"outputs":[],"source":["bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n","matches = bf.match(des_left,des_right)"]},{"cell_type":"markdown","metadata":{"id":"jCqctqL8bTL_"},"source":["OpenCV provides the `drawMatches` function to visualize the matches ([documentation](https://docs.opencv.org/3.4/d4/d5d/group__features2d__draw.html#ga7d77c42960a2916217ddf202173f9ed2))."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":496},"executionInfo":{"elapsed":6687,"status":"ok","timestamp":1678702393317,"user":{"displayName":"Luca Maiano","userId":"08981433972456241292"},"user_tz":-60},"id":"vKjqIFeabTL_","outputId":"081a1dfd-3da6-4fd3-c6e0-6b69273ab0e5"},"outputs":[],"source":["matches_drawn = cv2.drawMatches(left, kp_left, right, kp_right, matches, None, matchColor=(0,0,255), flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n","plot_cv2_images(matches_drawn)"]},{"cell_type":"markdown","metadata":{"id":"bOE-068UbTL_"},"source":["Viewing all matches can be of little use and a bit confusing. Because of this, we **sort** the best matches in ascending order of their distances so that the best matches (with low distance) come to the front. Then we draw only the **first 10 matches** (just for sake of visibility. You can increase it as you like)."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1678702393320,"user":{"displayName":"Luca Maiano","userId":"08981433972456241292"},"user_tz":-60},"id":"SAa2hwY2bTMA"},"outputs":[],"source":["limit = 10\n","best = sorted(matches, key = lambda x:x.distance)[:limit]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPjYqCJkbTMA"},"outputs":[],"source":["best_matches_drawn = cv2.drawMatches(left, kp_left, right, kp_right, best, None, matchColor=(0,0,255), flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n","plot_cv2_images(best_matches_drawn)"]},{"cell_type":"markdown","metadata":{"id":"xgh8uvqxbTMA"},"source":["## 3. Homography estimation using RANSAC and perspective warping\n","\n","Until now we were able to find some feature points in the images and find the best matches among them. Notice that these steps are typically also used for *image retrieval* applications in which, given a query image, we need to return all the images that are more similar to the query image (a bit like Google images).\n","\n","In our case, however, we want to transform the image on the right in order to stitch it with the one on the left. Nevertheless, the matcher algorithm will give us the best (more similar) set of features from both images. Now, we need to take these points and find the transformation matrix that will stitch the 2 images together based on their matching points. Such a transformation is called the **Homography matrix**. Briefly, the homography is a $3\\times3$ matrix that can be used in many applications such as camera pose estimation, perspective correction, and image stitching. The Homography is a 2D transformation.\n","\n","We will start by converting the best matches to coordinates on the left and right picture."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQcMZXyzbTMA"},"outputs":[],"source":["left_pts = []\n","right_pts = []\n","for match in best:\n","    l_match = kp_left[match.queryIdx].pt\n","    r_match = kp_right[match.trainIdx].pt\n","    left_pts.append(l_match)\n","    right_pts.append(r_match)"]},{"cell_type":"markdown","metadata":{"id":"dOR6NGoLbTMB"},"source":["Now we compute the transformation. Remind that the homography matrix is estimated with the **RANSAC algorithm**, which is an iterative algorithm to fit linear models and it's designed to be robust to outliers. We can use the `cv2.findHomography()` function ([documentation](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#ga4abc2ece9fab9398f2e560d53c8c9780))."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMbW4UG1bTMB"},"outputs":[],"source":["M, _ = cv2.findHomography(np.float32(right_pts), np.float32(left_pts))"]},{"cell_type":"markdown","metadata":{"id":"uq9B8UUubTMB"},"source":["Once we have the estimated homography, we need to warp one of the images to a common plane. Use the `cv2.warpPerspective()` function ([documentation](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#gaf73673a7e8e18ec6963e3774e6a94b87))."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RbqDX1S7bTMB"},"outputs":[],"source":["dim_x = left.shape[1] + right.shape[1]\n","dim_y = left.shape[0] + right.shape[0]\n","dim = (dim_x, dim_y)\n","\n","warped = cv2.warpPerspective(right, M, dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0UmuwUKcbTMB"},"outputs":[],"source":["plot_cv2_images(warped)"]},{"cell_type":"markdown","metadata":{"id":"LZVuk10QbTMC"},"source":["Finally, we warp the *left* image to the *right* based on the homography."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1sAcFzsCMQ3mTdHhcB4yiv4pzDwRYmQP4"},"id":"QaDay3KUbTMC","outputId":"586510e5-2051-4079-8358-0c887917deed"},"outputs":[],"source":["panorama = warped.copy()\n","# combine the two images\n","panorama[0:left.shape[0], 0:left.shape[1]] = left\n","\n","# crop\n","h_crop = max(left.shape[0], right.shape[0])\n","w_crop = 2800\n","panorama = panorama[:h_crop, :w_crop]\n","plot_cv2_images(panorama)"]},{"cell_type":"markdown","metadata":{"id":"ArzL-ytibTMC"},"source":["As we see, there are a couple of artifacts in the result related to lighting conditions and edge effects at the image boundaries. Ideally, we can perform post-processing techniques to normalize the intensities like **histogram matching**. This would likely make the result look more realistic! If you are interested, you can take it as an extra exercise. You can read [this article](https://towardsdatascience.com/histogram-matching-ee3a67b4cbc1) to learn more about histogram matching."]},{"cell_type":"markdown","metadata":{"id":"TBlskapVbTMC"},"source":["## References\n","\n","- [https://github.com/davidmasek/image_stitching](https://github.com/davidmasek/image_stitching)\n","- [https://towardsdatascience.com/image-panorama-stitching-with-opencv-2402bde6b46c](https://towardsdatascience.com/image-panorama-stitching-with-opencv-2402bde6b46c)"]}],"metadata":{"colab":{"name":"","version":""},"interpreter":{"hash":"20af6f5877404a39138c66414b69fd65fb2c91073a4c4da5739d700f5f27f004"},"kernelspec":{"display_name":"Python 3.9.7 ('vision')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"}},"nbformat":4,"nbformat_minor":0}
